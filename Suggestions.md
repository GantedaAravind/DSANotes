### âœ… **Sorting Algorithms**

- **Merge Sort**: Use when stable sort is needed; works well on linked lists or large datasets.
- **Quick Sort**: Fast average case; good for in-memory arrays, avoid for worst-case unless randomized.
- **Heap Sort**: Use when constant space is needed and stability is not important.
- **Counting/Radix Sort**: Use for integers with limited range.

---

### âœ… **Sorted Arrays**

- **Binary Search**: Use on sorted arrays to reduce time to `O(log n)`.

- When to Use Binary Search on Answers find the **optimal answer (min/max)** when the **search space is numeric and monotonic** (either increasing or decreasing behavior).

---

### âœ… **Prefix Sum**

- Use Prefix Sum when you need to quickly calculate the sum of elements in a subarray multiple times.

---

### âœ… **Hashing (HashMap / HashSet)**

- Use when fast insert, delete, and search (`O(1)` avg) is needed.
- Best for frequency maps, lookups, avoiding duplicates.

---

### âœ… **Heap (Priority Queue)**

- Use for top `k` problems (`n log k` instead of `n log n`).
- **Min-Heap**: For **k largest** elements.
- **Max-Heap**: For **k smallest** elements.
- Use in Dijkstraâ€™s, Huffman coding, or scheduling tasks.

---

### âœ… **Two Pointers / Sliding Window**

- Use for subarray/substring problems (e.g., max sum, longest window).
- Efficient in reducing nested loops to `O(n)`.

---

### âœ… **Greedy**

- Use when local optimal choices lead to global optimal (e.g., activity selection, Huffman coding).
- Usually simpler and faster if applicable.

---

### âœ… **Recursion & Backtracking**

- Use for exhaustive search in combinations, permutations, N-Queens, Sudoku.
- Backtracking = recursion with pruning.

---

### âœ… **Divide and Conquer**

- Use to break problem into subproblems: merge sort, quick sort, binary search.
- Efficient for large input sizes.

---

### âœ… **Dynamic Programming (DP)**

- Use when problem has **overlapping subproblems + optimal substructure**.
- Good for: Knapsack, DP on strings, LIS, LCS, etc.
- Use memoization (`top-down`) or tabulation (`bottom-up`).

---

### âœ… **Graphs**

- **DFS/BFS**: Traversal, connected components, shortest paths (unweighted).
- **Dijkstraâ€™s**: Shortest path in weighted graphs with non-negative weights.
- **Bellman-Ford**: Shortest path with negative weights.
- **Floyd-Warshall**: All-pairs shortest paths.
- **Union-Find (DSU)**: Cycle detection, Kruskalâ€™s MST, disjoint sets.

---

### âœ… **Trie**

- Use for prefix-based searching: dictionary, autocomplete, bitwise problems (XOR).
- Efficient for string sets or IP routing.

---

### âœ… **Segment Tree / Binary Indexed Tree (Fenwick Tree)**

- Use for **range queries** and **updates** (sum, min, max) in `O(log n)` time.
- Segment Tree: More flexible, supports complex operations.
- Fenwick Tree: More space-efficient for cumulative frequency.

---

### âœ… **Linked List**

- Use when frequent insert/delete from middle or ends are needed.
- Doubly linked list for LRU Cache.
- Use for dynamic memory when size is not known in advance.

---

### âœ… **Stack**

- Use for expression evaluation, backtracking, or tracking state (e.g., parentheses validation, histogram).
- Supports LIFO operations.

---

### âœ… **Queue / Deque**

- Queue: BFS, task scheduling (FIFO).
- Deque: Sliding window max/min, LRU cache.

---

## âœ… **Golden Rule of Thumb**

> **1 second â‰ˆ 10â· to 10â¸ operations** is safe in most online judges (like LeetCode).

This lets you **estimate which time complexity is safe**, based on input size.

---

### ğŸ§® Reference Table (Input Size vs Time Complexity)

| Max Input Size (`n`) | Safe Time Complexity          | Unsafe / TLE Risk  |
| -------------------- | ----------------------------- | ------------------ |
| â‰¤ 10                 | O(n!), O(2â¿), O(nÂ³)           | â€”                  |
| â‰¤ 20                 | O(2â¿), O(n!)                  | â€”                  |
| â‰¤ 100                | âœ… O(nÂ²)                      | âš ï¸ O(nÂ³), O(2â¿)    |
| â‰¤ 1,000              | âœ… O(nÂ²)                      | âš ï¸ O(nÂ³)           |
| â‰¤ 10â´                | âœ… O(nÂ·log n), O(nÂ²) (barely) | âš ï¸ O(nÂ²) may TLE   |
| â‰¤ 10âµ                | âœ… O(nÂ·log n), O(n)           | âŒ O(nÂ²) will TLE  |
| â‰¤ 10â¶                | âœ… O(n), O(nÂ·log n)           | âŒ O(nÂ²), O(nÂ¹â€¤âµ)  |
| â‰¤ 10â· or more        | âœ… O(n) only (linear)         | âŒ Anything slower |

---

### ğŸš¦ How to Use This?

#### Step 1: Read the constraints.

For example:

```txt
1 <= nums.length <= 10âµ
```

That tells you:

- **n is up to 10âµ**, so
- You **cannot afford O(nÂ²)**.
- You need **O(n)** or **O(nÂ·log n)** at worst.

---

#### Step 2: Match to time complexity

Use the table to guide:

- Use **HashMaps / Sets / Sorting (O(n log n))** if needed.
- Avoid nested loops unless guaranteed small input (like n â‰¤ 500).

---

### âœ… Example Decisions

#### Example 1:

```txt
n <= 10^5
```

â†’ **Only O(n log n)** or **O(n)** is safe.

- âœ… Sorting
- âœ… Prefix sum
- âœ… Sliding window
- âŒ Nested loops (O(nÂ²))

---

#### Example 2:

```txt
matrix of size â‰¤ 100 x 100
```

â†’ O(nÂ³) might be okay.

---

#### Example 3:

```txt
1 <= n <= 20
```

â†’ This is **very small** â†’ brute force or backtracking is fine.

- âœ… Bitmask DP
- âœ… O(2â¿)

---

### ğŸ§  Practice Tip

When you solve a problem:

1. **Estimate the number of operations** your code does.
2. Compare with the safe threshold: `10â· to 10â¸`.
3. If you're above it â†’ Time to optimize.

---
